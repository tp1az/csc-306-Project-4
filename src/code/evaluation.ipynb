{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rich\n",
    "import nbimporter\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Callable\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import zipfile\n",
    "\n",
    "from datasets import Dataset\n",
    "from databench_eval import Runner, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_hr = pd.read_parquet(\"../data/066_IBM_HR/sample.parquet\")\n",
    "tripadvisor = pd.read_parquet(\"../data/067_TripAdvisor/sample.parquet\")\n",
    "worldBank = pd.read_parquet(\"../data/068_WorldBank_Awards/sample.parquet\")\n",
    "taxonomy = pd.read_parquet(\"../data/069_Taxonomy/sample.parquet\")\n",
    "openfoodfacts = pd.read_parquet(\"../data/070_OpenFoodFacts/sample.parquet\")\n",
    "col = pd.read_parquet(\"../data/071_COL/sample.parquet\")\n",
    "admissions = pd.read_parquet(\"../data/072_Admissions/sample.parquet\")\n",
    "med_cost = pd.read_parquet(\"../data/073_Med_Cost/sample.parquet\")\n",
    "lift = pd.read_parquet(\"../data/074_Lift/sample.parquet\")\n",
    "mortality = pd.read_parquet(\"../data/075_Mortality/sample.parquet\")\n",
    "nba = pd.read_parquet(\"../data/076_NBA/sample.parquet\")\n",
    "gestational = pd.read_parquet(\"../data/077_Gestational/sample.parquet\")\n",
    "fires = pd.read_parquet(\"../data/078_Fires/sample.parquet\")\n",
    "coffee = pd.read_parquet(\"../data/079_Coffee/sample.parquet\")\n",
    "books = pd.read_parquet(\"../data/080_Books/sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns = ['DataSet','DataRaw'])\n",
    "data.loc[len(data)] = ['066_IBM_HR', ibm_hr.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['067_TripAdvisor', tripadvisor.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['068_WorldBank_Awards', worldBank.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['069_Taxonomy', taxonomy.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['070_OpenFoodFacts', openfoodfacts.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['071_COL', col.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['072_Admissions', admissions.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['073_Med_Cost', med_cost.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['074_Lift', lift.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['075_Mortality', mortality.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['076_NBA', nba.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['077_Gestational', gestational.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['078_Fires', fires.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['079_Coffee', coffee.to_csv(index=False)]\n",
    "data.loc[len(data)] = ['080_Books', books.to_csv(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgAge = (ibm_hr['Age'].sum() / ibm_hr['Age'].count())\n",
    "data = data.set_index('DataSet')\n",
    "query = \"Does the dataset contain any review that more than forty users have labeled as helpful?\"\n",
    "dataframe = data.loc['067_TripAdvisor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_context(df: pd.DataFrame) -> str:\n",
    "    summary = df.describe(include=\"all\").to_string()\n",
    "    return f\"Data Overview:\\n{summary}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def literal_context(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Converts the entire DataFrame to a structured text format.\"\"\"\n",
    "    return f\"Dataset:\\n{df.to_string(index=False)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_ctxt(df: str) -> str:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_zero_shot(model: str, question: str, context_method: Callable, data: pd.DataFrame) -> str:\n",
    "    context = context_method(data)\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a data analyst answering questions about a company dataset.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Dataset Summary: {context}\\n\\nQuestion: {question}\"},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the dataset contains a review from the user \"cmd123\" that has been labeled as helpful by 123 users.\n"
     ]
    }
   ],
   "source": [
    "result_df1 = query_llm_zero_shot(\"gpt-4o-mini\", query, ret_ctxt, dataframe['DataRaw'])\n",
    "print(result_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_qa = pd.read_csv(\"../data/test_qa.csv\")\n",
    "test_qa = test_qa.set_index('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "runner = 0\n",
    "last_name = \"\"\n",
    "final = []\n",
    "for name, query in test_qa.itertuples():\n",
    "    \n",
    "    if (runner % 10 == 0):\n",
    "        print(runner)\n",
    "    dataframe = data.loc[name]\n",
    "    result = query_llm_zero_shot(\"gpt-4o-mini\", query, ret_ctxt, dataframe['DataRaw'])\n",
    "    result = result.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "    final.append(result)\n",
    "    runner += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final[0].split(\":\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(final: list):\n",
    "    answers = []\n",
    "    for i in final:\n",
    "        ans = i.split(\":\")[1]\n",
    "        answers.append(ans.replace(\"\\n\", \"\").replace(\"'\", \"\").replace(\"```\",\"\").replace('\"',\"\").replace(\" \",\"\"))\n",
    "        return answers\n",
    "    \n",
    "    valid = clean(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert_string(s):\n",
    "    \"\"\"Convert a string to a list, boolean, or float if possible.\"\"\"\n",
    "    \n",
    "    # Try converting to a boolean first\n",
    "    if s.lower() == \"true\" or s.lower() == \"yes\" or s.lower() == \"y\":\n",
    "        return True\n",
    "    elif s.lower() == \"false\" or s.lower() == \"no\" or s.lower() == \"n\":\n",
    "        return False\n",
    "\n",
    "    # Try converting to a float\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Try converting to a list using ast.literal_eval\n",
    "    try:\n",
    "        if(len(s) != 0):\n",
    "            list_val = []\n",
    "            if s[0] == \"[\" and s[-1] == \"]\":\n",
    "                for i in s.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\"):\n",
    "                    list_val.append(convert_string(i))\n",
    "                return list_val\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    # If no conversion is possible, return the original string\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid = []\n",
    "for i in answers:\n",
    "    valid.append(convert_string(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(prompts: list[str]):\n",
    "    result_list = []\n",
    "    for p in prompts:\n",
    "        dataframe = data.loc[name]\n",
    "        result = query_llm_zero_shot(\"gpt-4o-mini\", query, ret_ctxt, dataframe['DataRaw'])\n",
    "        result = result.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "        result_list.append(result)\n",
    "    final = clean(result_list)\n",
    "    for i in final:\n",
    "        converted = convert_string(i)\n",
    "    return converted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Using Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ade2cfdf5241ed9a9c57ef9b6183b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6d83b13235412bb6341c67cc94fc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f6643a4ecf4762a0bdc5520e7609a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b988663f13d44f028128f0693655c521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "semeval_train_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"train\")\n",
    "semeval_dev_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qa = load_dataset(\"cardiffnlp/databench\", name=\"qa\", split=\"train\")\n",
    "print(all_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_df = pd.DataFrame()\n",
    "\n",
    "# Read the answers from the .txt files into separate lists\n",
    "with open(\"/Users/tonypiacentini/csc-306-Project-4/src/data/answers.txt\", \"r\") as f:\n",
    "    answers = f.read().splitlines()\n",
    "\n",
    "with open(\"/Users/tonypiacentini/csc-306-Project-4/src/data/answers_lite.txt\", \"r\") as f:\n",
    "    sample_answers = f.read().splitlines()\n",
    "\n",
    "with open(\"/Users/tonypiacentini/csc-306-Project-4/src/data/semantics.txt\", \"r\") as f:\n",
    "    semantics = f.read().splitlines()\n",
    "\n",
    "# Combine the lists into a DataFrame\n",
    "\n",
    "# Load the dataset column from the specified file\n",
    "test_qa[\"answer\"] = answers\n",
    "test_qa[\"sample_answer\"] = sample_answers\n",
    "test_qa[\"type\"] = semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa = Dataset.from_pandas(test_qa.head(100))\n",
    "evaluator = Evaluator(qa=qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DataBench_lite accuracy is {evaluator.eval(valid, lite=True)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
